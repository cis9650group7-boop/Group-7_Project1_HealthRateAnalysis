{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cis9650group7-boop/Group-7_Project1_HealthRateAnalysis/blob/main/term_project_group7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1: Analysis of Health Rate"
      ],
      "metadata": {
        "id": "B3aIe1AWf1FL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KlZ2o4ByS2Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XGP6kl5g7bX3",
        "outputId": "a8dcbfeb-ed59-44f4-b4df-f3e1d5969ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SUWyEn2xqn5W"
      },
      "cell_type": "markdown",
      "source": [
        "### Author: CIS 9650 Group 7 (Anish Bijusoman, Ivana Sundararao, Qingrong Tan, Reem Hussein)\n",
        "### Date : November 28th, 2025"
      ]
    },
    {
      "metadata": {
        "id": "vkBCfM4xqn5X"
      },
      "cell_type": "markdown",
      "source": [
        "## Executive Summary"
      ]
    },
    {
      "metadata": {
        "id": "nl7x4ox0qn5X"
      },
      "cell_type": "markdown",
      "source": []
    },
    {
      "metadata": {
        "id": "NQ3WFyKOqn5X"
      },
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ]
    },
    {
      "metadata": {
        "id": "XQlzOorTqn5X"
      },
      "cell_type": "markdown",
      "source": [
        "1. Introduction\n",
        "2. Problem Statement / Research Question\n",
        "3. Data Description\n",
        "4. Setup and Environment\n",
        "5. Data Loading\n",
        "6. Data Preparation\n",
        "7. Model Planning\n",
        "8. Model Building / Analysis\n",
        "9. Discussion & Interpretation\n",
        "10. Conclusion\n",
        "11. References\n",
        "12. Appendix"
      ]
    },
    {
      "metadata": {
        "id": "h6FSAiEmqn5Y"
      },
      "cell_type": "markdown",
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAlW921_rRvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "rNHFUNoMrZkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes doctor ratings in New York using a real-world dataset. Our goal is to understand how doctor quality varies across specialties and locations, and to identify meaningful patterns in patient satisfaction.  \n",
        "\n",
        "Because the dataset contains over 1,500 raw columns, we focused on selecting a clean and relevant subset of variables that support clear and actionable insights.  \n",
        "\n",
        "The analysis presented can help decision makers understand trends in physician ratings across New York State.\n"
      ],
      "metadata": {
        "id": "vPPsShEki5nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement / Research Question"
      ],
      "metadata": {
        "id": "gxlnArrBrm4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "0C3kZi5HrrSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Environment"
      ],
      "metadata": {
        "id": "PV0iyLXurt7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "mElIRgqfvDNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import plotly.express as px\n",
        "import json\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from io import BytesIO\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "Wd4ItUpruFe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "DMi7uYUhrx0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "kdcxnKu7vqbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def list_public_bucket(bucket_name, course, project):\n",
        "    prefix = f\"{course}/Project {project}/\"\n",
        "\n",
        "    client = storage.Client.create_anonymous_client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blobs = client.list_blobs(bucket, prefix=prefix)\n",
        "    print(f\"Listing files in public bucket '{bucket_name}' under '{prefix}':\")\n",
        "    filenames = []\n",
        "    for blob in blobs:\n",
        "        print(blob.name)\n",
        "        filenames.append(blob.name)\n",
        "\n",
        "    return filenames"
      ],
      "metadata": {
        "id": "1Wc3fjEBuFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import os\n",
        "import json\n",
        "\n",
        "def gcs_file_to_dataframe(bucket_name: str, blob_path: str, use_cols:list):\n",
        "\n",
        "    # Anonymous client for public buckets\n",
        "    client = storage.Client.create_anonymous_client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_path)\n",
        "\n",
        "    # Download file content as bytes\n",
        "    data = blob.download_as_bytes()\n",
        "\n",
        "    # Only read columns we use\n",
        "    df = pd.read_csv(BytesIO(data),usecols = use_cols)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "LPDXmA5cx5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set bucket parameters for \"msba-online-data/CIS9650/Project 01\"\n",
        "BUCKET_NAME = \"msba-online-data\"\n",
        "PROJECT_NUMBER = \"01\"\n",
        "COURSE = \"CIS9650\"\n",
        "healthrate_raw = list_public_bucket(BUCKET_NAME,COURSE,PROJECT_NUMBER)"
      ],
      "metadata": {
        "id": "6Vu5zWyRv1YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the csv file into a Pandas dataframe\n",
        "\n",
        "#List the columns needed to load\n",
        "columns_load = [\"_id\",\n",
        "                \"full_name\",\n",
        "                \"specialty_name\",\n",
        "                \"location.city.name\",\n",
        "                 \"rating.average\",\n",
        "                 \"rating.helpfulness\",\n",
        "                 \"rating.punctuality\",\n",
        "                  \"rating.staff\",\n",
        "                 \"rating.count\",\n",
        "                \"sample_rating_comment\"]\n",
        "\n",
        "csv_path = healthrate_raw[1]\n",
        "df = gcs_file_to_dataframe(BUCKET_NAME,  csv_path, columns_load)\n",
        "\n",
        "print(df.head())\n",
        "print(f'Rows:{len(df)}')\n"
      ],
      "metadata": {
        "id": "8ZzYd5BPxEeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Q9toqCgWr0Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The raw data set from the CSV file contained 1,611 columns and 143,791 rows. That in itself is too much to analyze, and also contained fields such as images, phone numbers, etc... Field that would be irrelivant to our analysis, and research question. So this calls for some data cleaning!\n",
        "\n",
        "* We selected a focused subset of relevant variables that describe each doctor’s name, specialty, city,core rating metrics, ect.\n",
        "\n",
        "* We renamed the columns to simpler names. Column names were written such as \"location.city.name\", \"rating.average\" so we changed them to \"city\" and \"rating_average\"\n",
        "\n",
        "* Rating fields were stored as mixed text/numeric types so we converted the rating columns to numeric using pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "\n",
        "* We filtered out zero rating doctors to make sure we only analyze doctors who have actual patient reviews in order to produce valid insights\n",
        "\n",
        "* After filtering, we reset the index for smooth readability."
      ],
      "metadata": {
        "id": "1BEkANPrToIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- DATA CLEANING ----------\n",
        "# Select usable columns\n",
        "cols_to_keep = [\n",
        "    \"_id\", \"full_name\", \"specialty_name\", \"location.city.name\",\n",
        "    \"rating.average\", \"rating.helpfulness\", \"rating.punctuality\",\n",
        "    \"rating.staff\", \"rating.count\",\"sample_rating_comment\"\n",
        "]\n",
        "\n",
        "df_clean = df[cols_to_keep].copy()\n",
        "\n",
        "# Rename columns to simpler names\n",
        "df_clean = df_clean.rename(columns={\n",
        "    \"location.city.name\": \"city\",\n",
        "    \"rating.average\": \"rating_avg\",\n",
        "    \"rating.helpfulness\": \"rating_help\",\n",
        "    \"rating.punctuality\": \"rating_punctuality\",\n",
        "    \"rating.staff\": \"rating_staff\",\n",
        "    \"rating.count\": \"rating_count\"\n",
        "})\n",
        "\n",
        "# Convert numeric columns to proper numeric types\n",
        "numeric_cols = [\n",
        "    \"rating_avg\", \"rating_help\", \"rating_punctuality\",\n",
        "    \"rating_staff\", \"rating_count\"\n",
        "]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with no rating information\n",
        "df_clean = df_clean.dropna(subset=[\"rating_avg\"])\n",
        "\n",
        "# Before having this the rating avg and rating_help was 0's\n",
        "df_clean = df_clean[df_clean[\"rating_count\"] > 0]\n",
        "df_clean = df_clean[df_clean[\"rating_avg\"] > 0]\n",
        "\n",
        "# Reset index\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "df_clean.head() # this is to see the clean data"
      ],
      "metadata": {
        "id": "gU4ifI7ATUbI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10) # this is unclean data"
      ],
      "metadata": {
        "id": "edmnmaL5uEmR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Planning\n",
        "### Palce codes for Agregate, Sum, Average, counts"
      ],
      "metadata": {
        "id": "2LakOsEXr3lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Planning for Insight 1: Specialty-Level Rating Variation and Insights from Patient Feedback\n"
      ],
      "metadata": {
        "id": "zNXItVt4DA-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Planning for Insight 1:Specialty-Level Rating Variation and Insights from Patient Feedback\n",
        "\n",
        "# ------- Calculations ------\n",
        "\n",
        "# 1.Create a working copy\n",
        "specialty_df = df_clean.copy()\n",
        "print(\"--- Starting Weighted Agrregation Calculation---\")\n",
        "\n",
        "# 2. Calculate Weighted Sum (Score Points)\n",
        "# Weighted Sum = Average Rating * Rating Count\n",
        "specialty_df['weighted_sum'] = specialty_df['rating_avg'] * specialty_df['rating_count']\n",
        "\n",
        "#3. Calculate the summary for each specialty\n",
        "specialty_grouped_df = specialty_df.groupby('specialty_name')\n",
        "specialty_result_df = specialty_grouped_df.agg(\n",
        "    total_score_sum =('weighted_sum','sum'),  # Total Score Points for a specialty\n",
        "    total_count = ('rating_count','sum'), # Toal rating_count for a specialty\n",
        "    num_doctors = ('_id','nunique') # Total numbers of doctors for a specialty\n",
        ")\n",
        "\n",
        "#4. Reset the index\n",
        "specialty_result_df = specialty_result_df.reset_index()\n",
        "\n",
        "#5. Calculate weighted average rating\n",
        "specialty_result_df['final_average_rating'] = specialty_result_df['total_score_sum'] / specialty_result_df['total_count']\n",
        "\n",
        "\n",
        "# ------ Filter out outliers (specialties with very low rating counts - bottom 10th percentile) ------\n",
        "\n",
        "# 1. Calculate the percentle threshold for total_count\n",
        "threshold_percentile = specialty_result_df['total_count'].quantile(0.25) #25%\n",
        "print(f\"The 25th percentile rating count threshold: {threshold_percentile}\")\n",
        "\n",
        "# 2. Apply the filter\n",
        "specialty_result_df = specialty_result_df[specialty_result_df['total_count'] >= threshold_percentile]\n",
        "\n",
        "# 3. Rank the remaning specialties (final_average_rating DESC).\n",
        "specialty_result_df = specialty_result_df.sort_values(by = 'final_average_rating', ascending = False)\n",
        "\n",
        "#print(specialty_result_df)\n"
      ],
      "metadata": {
        "id": "6OppN2NguEMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Planning for Inisght 2: Correlation between Helpfulness and Overall Rating"
      ],
      "metadata": {
        "id": "SdepHYzCie9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here:"
      ],
      "metadata": {
        "id": "YcnCb1zVinS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model building / Analysis"
      ],
      "metadata": {
        "id": "4CjKZrjUr5jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model building for Insight 1: Specialty-Level Rating Variation and Insights from Patient Feedback\n"
      ],
      "metadata": {
        "id": "VdNRAcNXD45R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model building for Insight 1: the differences in doctor ratings across different specialties\n",
        "# specialty_result_df is the dataframe already containing rows order by final_average_rating DESC\n",
        "\n",
        "# Top 10 Bar Chart\n",
        "# 1. Prepare Top 10 df\n",
        "top_10_df = specialty_result_df.head(10).copy()\n",
        "\n",
        "# 2. Sort the plotting data by rating ASC for correct chart display\n",
        "top_10_df = top_10_df.sort_values(by='final_average_rating', ascending=True)\n",
        "\n",
        "#3. Create Top 10 Horizontal Bar Chart\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x = 'final_average_rating',\n",
        "    y = 'specialty_name',\n",
        "    data = top_10_df,\n",
        "    color = 'deepskyblue'\n",
        ")\n",
        "plt.title('Figure 1: Top 10 Ranked Doctor Specialties (Average Rating)', fontsize = 14)\n",
        "plt.xlabel('Average Rating (out of 5)', fontsize = 11)\n",
        "plt.ylabel('Spectialty Name', fontsize = 11)\n",
        "\n",
        "# Add value for each bar\n",
        "for index, value in enumerate(top_10_df['final_average_rating']):\n",
        "  plt.text(\n",
        "      value + 0.005,\n",
        "      index,\n",
        "      f'{value:.3f}',\n",
        "      ha = 'left',\n",
        "      va = 'center',\n",
        "      fontsize = 10\n",
        "  )\n",
        "\n",
        "# Invert the y-axis to show the highest rated specialty at the top\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlim(top_10_df['final_average_rating'].min() * 0.99, 5.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Bottom 10 Bar Chart\n",
        "# 1. Prepare bottom 10 df\n",
        "bottom_10_df = specialty_result_df.tail(10).copy()\n",
        "\n",
        "# 2. Sort the plotting data by rating ASC for correct chart display\n",
        "bottom_10_df = bottom_10_df.sort_values(by='final_average_rating', ascending=True)\n",
        "\n",
        "#3. Create Bottom 10 Horizontal Bar Chart\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x = 'final_average_rating',\n",
        "    y = 'specialty_name',\n",
        "    data = bottom_10_df,\n",
        "    color = 'tomato'\n",
        ")\n",
        "plt.title('Figure 2: Bottom 10 Ranked Doctor Specialties (Average Rating)', fontsize = 14)\n",
        "plt.xlabel('Average Rating (out of 5)', fontsize = 11)\n",
        "plt.ylabel('Spectialty Name', fontsize = 11)\n",
        "\n",
        "# Add value for each bar\n",
        "for index, value in enumerate(bottom_10_df['final_average_rating']):\n",
        "  plt.text(\n",
        "      value + 0.005,\n",
        "      index,\n",
        "      f'{value:.3f}',\n",
        "      ha = 'left',\n",
        "      va = 'center',\n",
        "      fontsize = 10\n",
        "  )\n",
        "\n",
        "# Invert the y-axis to show the highest rated specialty at the top\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlim(bottom_10_df['final_average_rating'].min() * 0.99, 5.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cOzrfMmPuDTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis for Insight 1: What do patients talk about? (sample_rating_comment)"
      ],
      "metadata": {
        "id": "sCKbZQ_YmMTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of Top 10 Specialties' Positive Keywords\n",
        "\n",
        "# 1. Create a dataframe for comment analysis\n",
        "comments_df = df_clean.copy()\n",
        "\n",
        "# 2. Get the list of top 10 specialty name\n",
        "top_10_names = specialty_result_df.head(10)['specialty_name'].tolist()\n",
        "#print(f'Top 10 Specialties: {top_10_names}')\n",
        "\n",
        "# 3. Filter Data:\n",
        "top_10_reviews = comments_df[\n",
        "    (comments_df['specialty_name'].isin(top_10_names)) &\n",
        "    (comments_df['rating_avg'] >= 4.29) &\n",
        "    (comments_df['sample_rating_comment'].notna())\n",
        "\n",
        "    ]\n",
        "#print(f'Number of comments extracted: {len(top_10_reviews)}')\n",
        "\n",
        "# 4. Combine all comments into one large string\n",
        "all_text = ' '.join(top_10_reviews['sample_rating_comment'].astype(str).tolist())\n",
        "\n",
        "# 5. Convert to lowercase, replace any charaters that is not a word charater or a whitespace charater\n",
        "all_text = re.sub(r'[^\\w\\s]',' ', all_text).lower()\n",
        "\n",
        "# 6. A set of common words that need to ignore\n",
        "words_to_ignore = {'the','and','to','a','of','was','is','in','for','my','she','he','it','with','very','that',\n",
        "    'at','on','her','his','i','me','you','are','we','they','them','our','your','there','anyone','being',\n",
        "    'dr','doctor','doctors','patient','patients','office','staff','visit','appointment','time','day',\n",
        "    'year','years','person','issues','everything','health','thank','thanks','please',\n",
        "    'great','good','best','better','excellent','amazing','wonderful','fantastic','awesome','incredible',\n",
        "    'truly','happy','comfortable','really','so','too','quite','super','extremely','well','first','only','just',\n",
        "    'even','still','then','than','now','ever','never','enough','made','make','get','got','give','gave','take','took','felt','feel','went','came','see','seen','say',\n",
        "    'says','said','look','looks','know','knew','done','did','does','doing','help','helped','helps','able','need','needs','like','liked','likes','want','wanted','wanting',\n",
        "    'but','and','or','if','when','while','because','also','just','even','only','still','then','than',\n",
        "    'though','however','although','from','this','that','these','those','about','around','after','before',\n",
        "    'over','under','into','out','one','will','can','could','would','were','been','has','have','had','not','many','much','any','every','few','lot','lots','more','most','some','several','again',\n",
        "    'who','what','how','where','why','which','him','amp','work','having','seeing','found','treated','needed','without','visits','things','sure',\n",
        "    'during','since','ago','new','weeks','months','free','friends','everyone','team',\n",
        "    'feeling','all','highly','always','other','going','family','take','through','results','life','their','body','recommend','recommended','takes','way','right','thorough','ploblem',\t'both','don','two','times','long','each','entire','down','left','such',\n",
        "    'people','practice','job','love','easy','tell','come','gone','met','looking','explain','definitely','absolutely','next','makes','whole','should','process',\n",
        "    'lower','far','same','else','positive','saw','face','root','plastic','knowledgable','nice','questions','pain','back','surgery','treament','chiropractor','surgeon','neck','chiropractic','procedure','treatment','treatments',\n",
        "    'acupuncture','problems','problem','chiropractors','medical','breast','acupuncturist','hands','therapy','relief'\n",
        "    }\n",
        "\n",
        "# 7. Split\n",
        "comment_words = all_text.split()\n",
        "\n",
        "# 8. Filter: Keep words not in words_to_ignore and longer than 2 charaters\n",
        "meaningful_words = [mw for mw in comment_words if mw not in words_to_ignore and len(mw) > 2 ]\n",
        "\n",
        "# 9. Count the top 20 most frequent words\n",
        "word_counts = Counter(meaningful_words)\n",
        "top_keywords_df = pd.DataFrame(word_counts.most_common(20), columns = ['Keyword','Frequency'])\n",
        "#print(top_keywords_df)\n",
        "\n",
        "# 10. Chart\n",
        "sns.set_style('whitegrid')  # Set the visual style for the plot\n",
        "plt.figure(figsize=(10,8))  # Set the font size of the chart\n",
        "\n",
        "# Creat a horizontal bar plot\n",
        "ax = sns.barplot(\n",
        "    x = 'Frequency',\n",
        "    y = 'Keyword',\n",
        "    data = top_keywords_df,\n",
        "    color = 'mediumseagreen'\n",
        ")\n",
        "\n",
        "# Add title and lables\n",
        "plt.title('Top 20 Keywords in Positive Review (Top 10 Specialties)', fontsize = 15, fontweight = 'bold')\n",
        "plt.xlabel('Frequency Count',fontsize = 12)\n",
        "plt.ylabel('Keyword', fontsize = 12)\n",
        "\n",
        "# Add the frequency number at the end of each bar\n",
        "for inumber in ax.containers:\n",
        "  ax.bar_label(inumber,padding = 3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8_UzFC5umfmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis of Bottom 10 Specialties' Negative Keywords\n",
        "# 1. Create a dataframe for comment analysis\n",
        "comments_df2 = df_clean.copy()\n",
        "\n",
        "# 2. Get the list of bottom 10 specialty name\n",
        "bottom_10_names = specialty_result_df.tail(10)['specialty_name'].tolist()\n",
        "#print(f'Bottom 10 Specialties: {bottom_10_names}')\n",
        "\n",
        "# 3. Filter Data: Focus on Bottom 10 + Low Ratings (<= 2.5)\n",
        "bottom_10_reviews = comments_df2[\n",
        "    (comments_df2['specialty_name'].isin(bottom_10_names)) &\n",
        "    (comments_df2['rating_avg'] <= 2.5) &\n",
        "    (comments_df2['sample_rating_comment'].notna())\n",
        "]\n",
        "#print(f'Number of negative comments extracted: {len(bottom_10_reviews)}')\n",
        "\n",
        "# 4. Combine all comments into one large string\n",
        "all_text2 = ' '.join(bottom_10_reviews['sample_rating_comment'].astype(str).tolist())\n",
        "\n",
        "# 5. Convert to lowercase, replace any characters that is not a word character or a whitespace character\n",
        "all_text2 = re.sub(r'[^\\w\\s]',' ', all_text2).lower()\n",
        "\n",
        "# 6. A set of common words that need to ignore\n",
        "words_to_ignore2 = {\n",
        "    'the','and','to','a','of','was','is','in','for','my','she','he','it','with','very','that',\n",
        "    'at','on','her','his','i','me','you','are','we','they','them','our','your','there','anyone','being',\n",
        "    'dr','doctor','doctors','patient','patients','office','staff','visit','appointment','time','day',\n",
        "    'year','years','person','issues','everything','health','thank','thanks','please',\n",
        "    'truly','happy','comfortable','really','so','too','quite','super','extremely','well','first','only','just',\n",
        "    'even','still','then','than','now','ever','never','enough','made','make','get','got','give','gave','take','took','felt','feel','went','came','see','seen','say',\n",
        "    'says','said','look','looks','know','knew','done','did','does','doing','help','helped','helps','able','need','needs','like','liked','likes','want','wanted','wanting',\n",
        "    'but','or','if','when','while','because','also','though','however','although','from','this','these','those','about','around','after','before',\n",
        "    'over','under','into','out','one','will','can','could','would','were','been','has','have','had','not','many','much','any','every','few','lot','lots','more','most','some','several','again',\n",
        "    'who','what','how','where','why','which','him','amp','work','having','seeing','found','treated','needed','without','visits','things','sure',\n",
        "    'during','since','ago','new','weeks','months','free','friends','everyone','team',\n",
        "    'feeling','all','highly','always','other','going','family','through','results','life','their','body','recommend','recommended','takes','way','right','thorough',\n",
        "    'both','don','two','times','long','each','entire','down','left','such',\n",
        "    'people','practice','job','love','easy','tell','come','gone','met','looking','explain','definitely','absolutely','next','makes','whole','should','process',\n",
        "    'lower','far','same','else','saw','face','nice','questions','back','neck',\n",
        "    'medical','hands','care', 'another', 'away', 'experience', 'good', 'hospital', 'find', 'off',\n",
        "    'test', 'stay', 'later', 'child', 'blood', 'days', 'woman', 'better',\n",
        "    'someone', 'here', 'exam', 'prescribed', 'sent', 'son', 'medicine', 'tests',\n",
        "    'mother', 'nurse', 'symptoms','pain', 'medication', 'minutes', 'something', 'last', 'tried', 'man',\n",
        "    'appointments', 'pay', 'put', 'waited', 'physician', 'instead', 'ask',\n",
        "    'taking', 'meds', 'husband', 'diagnosis', 'wife', 'late', 'problem',\n",
        "    'actually', 'calls', 'wasn', 'side', 'hour', 'hours', 'think','room','nothing','surgery', 'different', 'speak', 'let', 'getting', 'completely',\n",
        "    'month', 'avoid', 'until', 'cold', 'front', 'seems', 'due',\n",
        "    'great', 'knowledge', 'old', 'once', 'seemed', 'talk',\n",
        "    'prescription', 'trying', 'problems', 'condition', 'place',\n",
        "    'myself', 'second', 'waste','finally', 'use', 'helpful', 'psychiatrist', 'leave', 'little', 'almost',\n",
        "    'medications', 'thing', 'making', 'information', 'point', 'baby', 'daughter',\n",
        "    'wouldn', 'believe', 'answer', 'week', 'cancer', 'check', 'run', 'emergency',\n",
        "    'cannot', 'couldn', 'infection', 'sick','professional','phone','treatment','anything', 'bedside', 'finally', 'concerns', 'receptionist',\n",
        "    'understand', 'less', 'talking', 'records', 'herself', 'service','thought', 'procedure', 'reviews', 'saying', 'may', 'asking',\n",
        "    'received', 'review', 'personal', 'least', 'working', 'given','three', 'bill', 'home', 'paid', 'fact', 'kept', 'lack', 'check','run', 'week', 'baby', 'daughter', 'little', 'almost',\n",
        "    'information', 'point', 'making','money','listen','follow','treat','worst','bad','poor','horrible','terrible','awful','mental','today','worse','taken','issue','manner','question','started','pregnancy','kind','care','diagnosed','caring'\n",
        "}\n",
        "\n",
        "# 7. Split\n",
        "comment_words2 = all_text2.split()\n",
        "\n",
        "# 8. Filter: Keep words not in words_to_ignore and longer than 2 characters\n",
        "meaningful_words2 = [mw2 for mw2 in comment_words2 if mw2 not in words_to_ignore2 and len(mw2) > 2 ]\n",
        "\n",
        "# 9. Count the top 20 most frequent words\n",
        "word_counts2 = Counter(meaningful_words2)\n",
        "bottom_keywords_df = pd.DataFrame(word_counts2.most_common(20), columns = ['Keyword','Frequency'])\n",
        "\n",
        "#print('\\nBottom 10 Specialties: Key Words in Negative Reviews')\n",
        "#print(bottom_keywords_df)\n",
        "\n",
        "# 10. Chart\n",
        "sns.set_style('whitegrid')  # Set the visual style for the plot\n",
        "plt.figure(figsize=(10,8))  # Set the font size of the chart\n",
        "\n",
        "# Creat a horizontal bar plot\n",
        "ax = sns.barplot(\n",
        "    x = 'Frequency',\n",
        "    y = 'Keyword',\n",
        "    data = bottom_keywords_df,\n",
        "    color = 'indianred'\n",
        ")\n",
        "\n",
        "# Add title and lables\n",
        "plt.title('Top 20 Keywords in Negative Review (Bottom 10 Specialties)', fontsize = 15, fontweight = 'bold')\n",
        "plt.xlabel('Frequency Count',fontsize = 12)\n",
        "plt.ylabel('Keyword', fontsize = 12)\n",
        "\n",
        "# Add the frequency number at the end of each bar\n",
        "for inumber in ax.containers:\n",
        "  ax.bar_label(inumber,padding = 3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jjnbTHybmfaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cagApXwmfF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRdii9o7uCw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion and Results"
      ],
      "metadata": {
        "id": "Fw_hs8cur9ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fMIDNj9YuAbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "fstKat9EsCOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9_S9Jo3tt9y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "wN9Uya8asFaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Barber, David. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012.\n",
        "2. Aste, Tomaso, Paola Cerchiello, and Roberta Scaramozzino. \"Information-Theoretic Causality Detection between Financial and Sentiment Data.\"Entropy, vol. 24, no. 6, 2022, pp. 1–18. DOI:10.3390/e24060774.\n",
        "3. Metz, Cade. \"Microsoft Puts OpenAI’s Sam Altman in Charge of New Advanced AI Research Team.\"\n",
        "The New York Times, 20 Nov. 2023, www.nytimes.com/2023/11/20/technology/openai-microsoft-altman.html"
      ],
      "metadata": {
        "id": "iqLZY_rAt-TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix"
      ],
      "metadata": {
        "id": "ZT5TPWb9sHws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vIxXm5Dlt_Uf"
      }
    }
  ]
}